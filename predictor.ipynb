{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ba0e923d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import json\n",
    "import tiktoken\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class GPTConfig:\n",
    "#     block_size: int = 512 # 文本最大长度，max_seq\n",
    "#     batch_size: int = 12\n",
    "#     n_layer: int = 12\n",
    "#     n_head: int = 12\n",
    "#     n_embd: int = 768     # hidden_dim, hidden_size; 此处emb_size\n",
    "#     hidden_dim: int = n_embd\n",
    "#     # 为了 tie_embedding_weight\n",
    "#     dropout: float = 0.1\n",
    "#     head_size: int = n_embd // n_head\n",
    "#     # vocab_size\n",
    "#     # 和GPT2官方tokenizer\n",
    "#     vocab_size: int = 50527\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # 减小模型配置\n",
    "    block_size: int = 256  # 从512减至256\n",
    "    batch_size: int = 4    # 从12减至4\n",
    "    n_layer: int = 6       # 从12减至6\n",
    "    n_head: int = 8        # 从12减至8\n",
    "    n_embd: int = 384      # 从768减至384\n",
    "    hidden_dim: int = n_embd\n",
    "    dropout: float = 0.5\n",
    "    head_size: int = n_embd // n_head\n",
    "    vocab_size: int = 50527"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structure of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Single head attention\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.value = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.query = nn.Linear(config.hidden_dim, config.head_size)\n",
    "        self.head_size = config.head_size\n",
    "\n",
    "        # 用 register_buffer 注册 attention_mask\n",
    "        # 不用计算梯度，节约内存和显存\n",
    "        self.register_buffer(\n",
    "            \"attention_mask\",\n",
    "            # tril 是下三角\n",
    "            # block_size 文本最大长度, 512\n",
    "            torch.tril(\n",
    "                torch.ones(config.block_size, config.block_size)\n",
    "            )\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        weight = q @ k.transpose(-2, -1)  # @ 是 torch.matmul\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0,\n",
    "            float('-inf')\n",
    "        )\n",
    "        # 注意计算 weight 时要除以 sqrt(d_k)\n",
    "        weight = F.softmax(weight, dim=-1) / math.sqrt(self.head_size)\n",
    "        \n",
    "        # dropout 要放在weight后\n",
    "        weight = self.dropout(weight)\n",
    "        output = weight @ v\n",
    "        return output\n",
    "    \n",
    "# 2. Multi head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.heads =  nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(config)\n",
    "                for _ in range(config.n_head)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(config.hidden_dim, config.hidden_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in self.heads],\n",
    "            dim = -1\n",
    "        )\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "    \n",
    "# 3. feed forward(MLP)\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(config.hidden_dim, 4 * config.hidden_dim),  # swiglu # up to 8/3\n",
    "#             nn.GELU,\n",
    "#             nn.Linear(4 * config.hidden_dim, config.hidden_dim),\n",
    "#             nn.Dropout(config.dropout)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),  # <-- Fixed comma here\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# 4. block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# 5. GPT\n",
    "# # class GPT(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         # (embedding, position, norm, mlp, block)\n",
    "#         # position embedding 从0，1, ...embedding -> rope\n",
    "#         # norm   layer norm -> rms norm\n",
    "#         # mlp -> swiglu\n",
    "#         # mha -> gqa\n",
    "#         self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "#         self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "#         self.blocks = nn.Sequential(\n",
    "#             *[Block(config) for _ in range(config.n_layer)]\n",
    "#         )\n",
    "#         self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "#         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "#         # now SLM use tie weight to lessen parameters\n",
    "\n",
    "#         # linear (4->8), weight实际上的shape是8*4\n",
    "#         self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "#     def _init_weights(self, module):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             # 初始化为高斯分布\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "#             if module.bias is not None:\n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "#             elif isinstance(module, nn.Embedding):\n",
    "#                 torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "#     def forward(self, idx, targets=None):\n",
    "#         # idx 输入是 token ids\n",
    "#         # targets 是目标 token ids\n",
    "#         # shape 一样\n",
    "#         batch, seq_len = idx.size()  # (batch, seq_len)\n",
    "#         token_emb = self.token_embedding_table(idx)  # (batch, seq_len, n_embd)\n",
    "#         pos_emb = self.position_embedding_table(\n",
    "#             torch.arange(seq_len, device=idx.device)\n",
    "#         )\n",
    "#         # token_embedding, position_embedding 可以相加\n",
    "#         x = token_emb + pos_emb\n",
    "#         x = self.blocks(x)\n",
    "#         x = self.ln_final(x)\n",
    "#         logits = self.lm_head(x)\n",
    "#         if targets is None:\n",
    "#             loss = None\n",
    "#         else:\n",
    "#             batch, seq_len, vocab_size = logits.size()\n",
    "#             logits = logits.view(batch * seq_len, vocab_size)\n",
    "#             targets = targets.view(batch * seq_len)\n",
    "#             loss = F.cross_entropy(logits, targets)\n",
    "#         return logits, loss\n",
    "    \n",
    "#     def generate(self, idx, max_new_tokens):\n",
    "#         pass\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "        # 修改输出层以匹配emoji分类任务\n",
    "        self.classifier = nn.Linear(config.n_embd, len(dataset.label_encoder.classes_))\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()  # batch_size, sequence_length\n",
    "        \n",
    "        # 1. 获取token和位置嵌入\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = token_emb + pos_emb\n",
    "        \n",
    "        # 2. 通过transformer blocks\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        \n",
    "        # 3. 对序列进行池化，取平均值作为文本表示\n",
    "        x = x.mean(dim=1)  # [B, n_embd]\n",
    "        \n",
    "        # 4. 通过分类器得到emoji预测\n",
    "        logits = self.classifier(x)  # [B, num_classes]\n",
    "        \n",
    "        # 5. 如果提供了目标，计算损失\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # 生成功能可以根据需要实现\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. construct the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## understanding the input of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, path, block_size=512):\n",
    "\n",
    "#         import tiktoken\n",
    "#         self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "#         self.block_size = block_size  # pos最大长度\n",
    "\n",
    "#         self.encoded_data = []\n",
    "#         # <|endoftext|> 分割不同文本\n",
    "#         self.eos_token = self.enc.encode(\n",
    "#             \"<|endoftext|>\",\n",
    "#             allowed_special={\"<|endoftext|>\"}\n",
    "#         )[0]\n",
    "        \n",
    "#         self.max_lines = 1000\n",
    "#         import json\n",
    "\n",
    "#         raw_data = []\n",
    "#         with open(path, 'r') as f:\n",
    "#             for i, line in enumerate(f):\n",
    "#                 if i >= self.max_lines:\n",
    "#                     break\n",
    "#                 try:\n",
    "#                     text = json.loads(line.strip())[\"text\"]\n",
    "#                     raw_data.append(text)\n",
    "#                 except Exception as e:\n",
    "#                     continue\n",
    "\n",
    "#         full_encoded =  []\n",
    "#         for text in raw_data:\n",
    "#             encoded_text = self.enc.encode(text)\n",
    "#             full_encoded.extend(encoded_text + [self.eos_token])\n",
    "\n",
    "#         # block_size is 512\n",
    "#         # 长 -> 短(512)\n",
    "#         for i in range(0, len(full_encoded), self.block_size):\n",
    "#             chunk = full_encoded[i:i+self.block_size+1] # 512 实际上是513\n",
    "#             if len(chunk) < self.block_size + 1:\n",
    "#                 chunk = chunk + [self.eos_token] * (self.block_size + 1 - len(chunk))\n",
    "#             self.encoded_data.append(chunk)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.encoded_data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         chunk = self.encoded_data[idx]\n",
    "#         x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "#         y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "#         return x, y\n",
    "    \n",
    "#     def encode(self, text):\n",
    "#         \"\"\"将文本编码为token IDs\"\"\"\n",
    "#         return self.enc.encode(text)\n",
    "    \n",
    "#     def decode(self, ids):\n",
    "#         \"\"\"将token IDs解码为文本\"\"\"\n",
    "#         return self.enc.decode(ids)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path, block_size=512):\n",
    "        self.block_size = block_size\n",
    "        self.texts = []\n",
    "        self.emojis = []\n",
    "        \n",
    "        # 加载数据集\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                self.texts.append(item[\"text\"])\n",
    "                self.emojis.append(item[\"emoji\"])\n",
    "\n",
    "        # 将表情符号转换为整数标签\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.emoji_labels = self.label_encoder.fit_transform(self.emojis)\n",
    "        \n",
    "        # 初始化tokenizer\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        emoji_label = self.emoji_labels[idx]\n",
    "\n",
    "        # 将文本编码为token IDs\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "\n",
    "        # 处理序列长度\n",
    "        if len(token_ids) > self.block_size:\n",
    "            token_ids = token_ids[:self.block_size]\n",
    "        else:\n",
    "            # 填充到固定长度\n",
    "            token_ids = token_ids + [self.tokenizer.eot_token] * (self.block_size - len(token_ids))\n",
    "\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(emoji_label, dtype=torch.long)\n",
    "\n",
    "    def get_emoji_mapping(self):\n",
    "        return dict(zip(self.label_encoder.classes_, \n",
    "                       self.label_encoder.transform(self.label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GPT(GPTConfig())\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = model.to(device)\n",
    "\n",
    "# # 打印模型参数量\n",
    "\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Total parameters: {total_params / 1e6} M\")\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "# #设置cosine学习率\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "\n",
    "# 分布式训练设置函数\n",
    "def setup_ddp(rank, world_size):\n",
    "    \"\"\"初始化DDP设置\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup_ddp():\n",
    "    \"\"\"清理DDP进程组\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# 分布式训练函数\n",
    "def train_ddp(rank, world_size, config):\n",
    "    # 设置DDP\n",
    "    setup_ddp(rank, world_size)\n",
    "    \n",
    "    # 初始化数据集\n",
    "    dataset = MyDataset(path=\"dataset.json\", block_size=config.block_size)\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # 创建分布式采样器\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)\n",
    "    \n",
    "    # 创建DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        sampler=val_sampler,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = GPT(config).to(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # 初始化优化器和学习率调度器\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # 训练循环\n",
    "    num_epochs = 3\n",
    "    for epoch in range(num_epochs):\n",
    "        train_sampler.set_epoch(epoch)  # 确保每个epoch的数据分布不同\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (token_ids, emoji_labels) in enumerate(train_loader):\n",
    "            token_ids = token_ids.to(rank)\n",
    "            emoji_labels = emoji_labels.to(rank)\n",
    "            \n",
    "            with autocast():\n",
    "                logits, loss = model(token_ids, targets=emoji_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if rank == 0 and batch_idx % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for token_ids, emoji_labels in val_loader:\n",
    "                token_ids = token_ids.to(rank)\n",
    "                emoji_labels = emoji_labels.to(rank)\n",
    "                with autocast():\n",
    "                    logits, loss = model(token_ids, targets=emoji_labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f'Epoch: {epoch + 1}, Train Loss: {total_loss/len(train_loader):.4f}, '\n",
    "                  f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "            \n",
    "            # 保存模型\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.module.state_dict(),  # 注意这里使用model.module\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': val_loss / len(val_loader)\n",
    "            }\n",
    "            torch.save(checkpoint, f'model_epoch_{epoch + 1}.pt')\n",
    "    \n",
    "    cleanup_ddp()\n",
    "\n",
    "# 单GPU训练函数\n",
    "def train_single_gpu(gpu_id=0):\n",
    "    # 设置要使用的GPU\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    device = torch.device(f'cuda:{gpu_id}')\n",
    "    \n",
    "    config = GPTConfig()\n",
    "    \n",
    "    # 初始化数据集和数据加载器\n",
    "    dataset = MyDataset(path=\"dataset.json\", block_size=config.block_size)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = GPT(config).to(device)\n",
    "    \n",
    "    # 初始化优化器等\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # 训练循环\n",
    "    num_epochs = 30\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (token_ids, emoji_labels) in enumerate(train_loader):\n",
    "            token_ids = token_ids.to(device)\n",
    "            emoji_labels = emoji_labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                logits, loss = model(token_ids, targets=emoji_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for token_ids, emoji_labels in val_loader:\n",
    "                token_ids = token_ids.to(device)\n",
    "                emoji_labels = emoji_labels.to(device)\n",
    "                with autocast():\n",
    "                    logits, loss = model(token_ids, targets=emoji_labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch: {epoch + 1}, Train Loss: {total_loss/len(train_loader):.4f}, '\n",
    "              f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "        \n",
    "        # 保存模型\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss / len(val_loader)\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint/model_epoch_{epoch + 1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 3.9482\n",
      "Epoch: 0, Batch: 10, Loss: 3.6270\n",
      "Epoch: 0, Batch: 20, Loss: 3.3979\n",
      "Epoch: 1, Train Loss: 4.0807, Val Loss: 4.3141\n",
      "Epoch: 1, Batch: 0, Loss: 3.6963\n",
      "Epoch: 1, Batch: 10, Loss: 4.2041\n",
      "Epoch: 1, Batch: 20, Loss: 4.4878\n",
      "Epoch: 2, Train Loss: 3.7246, Val Loss: 4.6834\n",
      "Epoch: 2, Batch: 0, Loss: 3.5615\n",
      "Epoch: 2, Batch: 10, Loss: 2.9434\n",
      "Epoch: 2, Batch: 20, Loss: 3.1699\n",
      "Epoch: 3, Train Loss: 3.6081, Val Loss: 4.8612\n",
      "Epoch: 3, Batch: 0, Loss: 2.4219\n",
      "Epoch: 3, Batch: 10, Loss: 3.4126\n",
      "Epoch: 3, Batch: 20, Loss: 4.1592\n",
      "Epoch: 4, Train Loss: 3.4786, Val Loss: 4.5826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 选择训练模式：\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. 使用单个GPU（例如GPU 1）\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_single_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 199\u001b[0m, in \u001b[0;36mtrain_single_gpu\u001b[0;34m(gpu_id)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[1;32m    192\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n\u001b[1;32m    198\u001b[0m }\n\u001b[0;32m--> 199\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint/model_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/siluyang/miniconda3/envs/env_py39/lib/python3.9/site-packages/torch/serialization.py:442\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n",
      "File \u001b[0;32m/data/siluyang/miniconda3/envs/env_py39/lib/python3.9/site-packages/torch/serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 选择训练模式：\n",
    "\n",
    "# 1. 使用单个GPU（例如GPU 1）\n",
    "train_single_gpu(gpu_id=1)\n",
    "\n",
    "# 或者\n",
    "\n",
    "# 2. 使用所有可用GPU进行分布式训练\n",
    "# def main_ddp():\n",
    "#     world_size = torch.cuda.device_count()  # 获取可用的GPU数量\n",
    "#     config = GPTConfig()\n",
    "#     mp.spawn(\n",
    "#         train_ddp,\n",
    "#         args=(world_size, config),\n",
    "#         nprocs=world_size,\n",
    "#         join=True\n",
    "#     )\n",
    "\n",
    "# main_ddp()  # 取消注释此行来使用多GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. explanation of the checkpoint.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint信息:\n",
      "--------------------------------------------------\n",
      "Epoch: 1\n",
      "Validation Loss: 4.1938\n",
      "\n",
      "模型参数信息:\n",
      "--------------------------------------------------\n",
      "token_embedding_table.weight: shape [50527, 384], parameters: 19,402,368\n",
      "position_embedding_table.weight: shape [256, 384], parameters: 98,304\n",
      "blocks.0.att.heads.0.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.0.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.0.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.0.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.0.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.0.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.0.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.1.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.1.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.1.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.1.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.1.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.1.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.1.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.2.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.2.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.2.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.2.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.2.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.2.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.2.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.3.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.3.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.3.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.3.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.3.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.3.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.3.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.4.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.4.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.4.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.4.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.4.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.4.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.4.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.5.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.5.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.5.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.5.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.5.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.5.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.5.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.6.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.6.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.6.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.6.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.6.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.6.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.6.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.7.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.0.att.heads.7.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.7.key.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.7.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.7.value.bias: shape [48], parameters: 48\n",
      "blocks.0.att.heads.7.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.0.att.heads.7.query.bias: shape [48], parameters: 48\n",
      "blocks.0.att.proj.weight: shape [384, 384], parameters: 147,456\n",
      "blocks.0.att.proj.bias: shape [384], parameters: 384\n",
      "blocks.0.ffn.net.0.weight: shape [1536, 384], parameters: 589,824\n",
      "blocks.0.ffn.net.0.bias: shape [1536], parameters: 1,536\n",
      "blocks.0.ffn.net.2.weight: shape [384, 1536], parameters: 589,824\n",
      "blocks.0.ffn.net.2.bias: shape [384], parameters: 384\n",
      "blocks.0.ln1.weight: shape [384], parameters: 384\n",
      "blocks.0.ln1.bias: shape [384], parameters: 384\n",
      "blocks.0.ln2.weight: shape [384], parameters: 384\n",
      "blocks.0.ln2.bias: shape [384], parameters: 384\n",
      "blocks.1.att.heads.0.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.0.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.0.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.0.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.0.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.0.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.0.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.1.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.1.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.1.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.1.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.1.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.1.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.1.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.2.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.2.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.2.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.2.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.2.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.2.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.2.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.3.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.3.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.3.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.3.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.3.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.3.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.3.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.4.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.4.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.4.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.4.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.4.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.4.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.4.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.5.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.5.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.5.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.5.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.5.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.5.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.5.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.6.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.6.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.6.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.6.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.6.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.6.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.6.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.7.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.1.att.heads.7.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.7.key.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.7.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.7.value.bias: shape [48], parameters: 48\n",
      "blocks.1.att.heads.7.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.1.att.heads.7.query.bias: shape [48], parameters: 48\n",
      "blocks.1.att.proj.weight: shape [384, 384], parameters: 147,456\n",
      "blocks.1.att.proj.bias: shape [384], parameters: 384\n",
      "blocks.1.ffn.net.0.weight: shape [1536, 384], parameters: 589,824\n",
      "blocks.1.ffn.net.0.bias: shape [1536], parameters: 1,536\n",
      "blocks.1.ffn.net.2.weight: shape [384, 1536], parameters: 589,824\n",
      "blocks.1.ffn.net.2.bias: shape [384], parameters: 384\n",
      "blocks.1.ln1.weight: shape [384], parameters: 384\n",
      "blocks.1.ln1.bias: shape [384], parameters: 384\n",
      "blocks.1.ln2.weight: shape [384], parameters: 384\n",
      "blocks.1.ln2.bias: shape [384], parameters: 384\n",
      "blocks.2.att.heads.0.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.0.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.0.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.0.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.0.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.0.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.0.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.1.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.1.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.1.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.1.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.1.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.1.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.1.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.2.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.2.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.2.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.2.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.2.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.2.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.2.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.3.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.3.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.3.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.3.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.3.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.3.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.3.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.4.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.4.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.4.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.4.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.4.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.4.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.4.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.5.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.5.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.5.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.5.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.5.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.5.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.5.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.6.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.6.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.6.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.6.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.6.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.6.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.6.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.7.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.2.att.heads.7.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.7.key.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.7.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.7.value.bias: shape [48], parameters: 48\n",
      "blocks.2.att.heads.7.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.2.att.heads.7.query.bias: shape [48], parameters: 48\n",
      "blocks.2.att.proj.weight: shape [384, 384], parameters: 147,456\n",
      "blocks.2.att.proj.bias: shape [384], parameters: 384\n",
      "blocks.2.ffn.net.0.weight: shape [1536, 384], parameters: 589,824\n",
      "blocks.2.ffn.net.0.bias: shape [1536], parameters: 1,536\n",
      "blocks.2.ffn.net.2.weight: shape [384, 1536], parameters: 589,824\n",
      "blocks.2.ffn.net.2.bias: shape [384], parameters: 384\n",
      "blocks.2.ln1.weight: shape [384], parameters: 384\n",
      "blocks.2.ln1.bias: shape [384], parameters: 384\n",
      "blocks.2.ln2.weight: shape [384], parameters: 384\n",
      "blocks.2.ln2.bias: shape [384], parameters: 384\n",
      "blocks.3.att.heads.0.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.0.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.0.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.0.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.0.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.0.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.0.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.1.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.1.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.1.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.1.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.1.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.1.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.1.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.2.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.2.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.2.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.2.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.2.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.2.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.2.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.3.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.3.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.3.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.3.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.3.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.3.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.3.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.4.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.4.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.4.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.4.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.4.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.4.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.4.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.5.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.5.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.5.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.5.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.5.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.5.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.5.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.6.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.6.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.6.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.6.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.6.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.6.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.6.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.7.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.3.att.heads.7.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.7.key.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.7.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.7.value.bias: shape [48], parameters: 48\n",
      "blocks.3.att.heads.7.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.3.att.heads.7.query.bias: shape [48], parameters: 48\n",
      "blocks.3.att.proj.weight: shape [384, 384], parameters: 147,456\n",
      "blocks.3.att.proj.bias: shape [384], parameters: 384\n",
      "blocks.3.ffn.net.0.weight: shape [1536, 384], parameters: 589,824\n",
      "blocks.3.ffn.net.0.bias: shape [1536], parameters: 1,536\n",
      "blocks.3.ffn.net.2.weight: shape [384, 1536], parameters: 589,824\n",
      "blocks.3.ffn.net.2.bias: shape [384], parameters: 384\n",
      "blocks.3.ln1.weight: shape [384], parameters: 384\n",
      "blocks.3.ln1.bias: shape [384], parameters: 384\n",
      "blocks.3.ln2.weight: shape [384], parameters: 384\n",
      "blocks.3.ln2.bias: shape [384], parameters: 384\n",
      "blocks.4.att.heads.0.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.0.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.0.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.0.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.0.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.0.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.0.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.1.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.1.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.1.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.1.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.1.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.1.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.1.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.2.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.2.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.2.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.2.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.2.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.2.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.2.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.3.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.3.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.3.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.3.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.3.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.3.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.3.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.4.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.4.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.4.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.4.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.4.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.4.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.4.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.5.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.5.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.5.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.5.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.5.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.5.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.5.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.6.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.6.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.6.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.6.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.6.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.6.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.6.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.7.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.4.att.heads.7.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.7.key.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.7.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.7.value.bias: shape [48], parameters: 48\n",
      "blocks.4.att.heads.7.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.4.att.heads.7.query.bias: shape [48], parameters: 48\n",
      "blocks.4.att.proj.weight: shape [384, 384], parameters: 147,456\n",
      "blocks.4.att.proj.bias: shape [384], parameters: 384\n",
      "blocks.4.ffn.net.0.weight: shape [1536, 384], parameters: 589,824\n",
      "blocks.4.ffn.net.0.bias: shape [1536], parameters: 1,536\n",
      "blocks.4.ffn.net.2.weight: shape [384, 1536], parameters: 589,824\n",
      "blocks.4.ffn.net.2.bias: shape [384], parameters: 384\n",
      "blocks.4.ln1.weight: shape [384], parameters: 384\n",
      "blocks.4.ln1.bias: shape [384], parameters: 384\n",
      "blocks.4.ln2.weight: shape [384], parameters: 384\n",
      "blocks.4.ln2.bias: shape [384], parameters: 384\n",
      "blocks.5.att.heads.0.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.0.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.0.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.0.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.0.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.0.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.0.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.1.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.1.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.1.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.1.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.1.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.1.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.1.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.2.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.2.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.2.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.2.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.2.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.2.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.2.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.3.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.3.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.3.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.3.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.3.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.3.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.3.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.4.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.4.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.4.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.4.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.4.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.4.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.4.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.5.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.5.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.5.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.5.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.5.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.5.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.5.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.6.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.6.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.6.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.6.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.6.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.6.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.6.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.7.attention_mask: shape [256, 256], parameters: 65,536\n",
      "blocks.5.att.heads.7.key.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.7.key.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.7.value.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.7.value.bias: shape [48], parameters: 48\n",
      "blocks.5.att.heads.7.query.weight: shape [48, 384], parameters: 18,432\n",
      "blocks.5.att.heads.7.query.bias: shape [48], parameters: 48\n",
      "blocks.5.att.proj.weight: shape [384, 384], parameters: 147,456\n",
      "blocks.5.att.proj.bias: shape [384], parameters: 384\n",
      "blocks.5.ffn.net.0.weight: shape [1536, 384], parameters: 589,824\n",
      "blocks.5.ffn.net.0.bias: shape [1536], parameters: 1,536\n",
      "blocks.5.ffn.net.2.weight: shape [384, 1536], parameters: 589,824\n",
      "blocks.5.ffn.net.2.bias: shape [384], parameters: 384\n",
      "blocks.5.ln1.weight: shape [384], parameters: 384\n",
      "blocks.5.ln1.bias: shape [384], parameters: 384\n",
      "blocks.5.ln2.weight: shape [384], parameters: 384\n",
      "blocks.5.ln2.bias: shape [384], parameters: 384\n",
      "ln_final.weight: shape [384], parameters: 384\n",
      "ln_final.bias: shape [384], parameters: 384\n",
      "classifier.weight: shape [53, 384], parameters: 20,352\n",
      "classifier.bias: shape [53], parameters: 53\n",
      "\n",
      "总参数量: 33,314,357\n",
      "\n",
      "优化器信息:\n",
      "--------------------------------------------------\n",
      "优化器类型: 2.99e-04 (学习率)\n",
      "\n",
      "学习率调度器信息:\n",
      "--------------------------------------------------\n",
      "Last epoch: 27\n"
     ]
    }
   ],
   "source": [
    "def inspect_checkpoint(checkpoint_path):\n",
    "    \"\"\"\n",
    "    检查保存的模型checkpoint文件\n",
    "    参数:\n",
    "        checkpoint_path: 模型文件路径，如 'model_epoch_1.pt'\n",
    "    \"\"\"\n",
    "    # 加载checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"Checkpoint信息:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 打印基本信息\n",
    "    print(f\"Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    \n",
    "    # 检查模型状态字典\n",
    "    model_state = checkpoint['model_state_dict']\n",
    "    print(\"\\n模型参数信息:\")\n",
    "    print(\"-\" * 50)\n",
    "    total_params = 0\n",
    "    for name, param in model_state.items():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        print(f\"{name}: shape {list(param.shape)}, parameters: {param_count:,}\")\n",
    "    print(f\"\\n总参数量: {total_params:,}\")\n",
    "    \n",
    "    # 检查优化器状态\n",
    "    optimizer_state = checkpoint['optimizer_state_dict']\n",
    "    print(\"\\n优化器信息:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"优化器类型: {optimizer_state['param_groups'][0]['lr']:.2e} (学习率)\")\n",
    "    \n",
    "    # 检查学习率调度器状态\n",
    "    scheduler_state = checkpoint['scheduler_state_dict']\n",
    "    print(\"\\n学习率调度器信息:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Last epoch: {scheduler_state['last_epoch']}\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "# 使用示例:\n",
    "# 查看某个特定epoch的模型\n",
    "checkpoint_path = 'model_epoch_1.pt'  # 更改为你想查看的文件名\n",
    "checkpoint = inspect_checkpoint(checkpoint_path)\n",
    "\n",
    "# 如果想加载模型进行预测，可以这样做：\n",
    "def load_model_for_inference(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    加载模型用于推理\n",
    "    \"\"\"\n",
    "    # 创建模型实例\n",
    "    config = GPTConfig()\n",
    "    model = GPT(config)\n",
    "    \n",
    "    # 加载checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # 加载模型权重\n",
    "    if 'module.state_dict' in checkpoint['model_state_dict']:\n",
    "        # 如果是从DDP模型保存的\n",
    "        model.load_state_dict({k[7:]: v for k, v in checkpoint['model_state_dict'].items()})\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()  # 设置为评估模式\n",
    "    return model\n",
    "\n",
    "# 使用示例:\n",
    "# model = load_model_for_inference('model_epoch_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. start our inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1309\n",
      "🎇: 0.1007\n",
      "😭: 0.0815\n",
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1196\n",
      "🎇: 0.0948\n",
      "😭: 0.0872\n",
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1206\n",
      "🎇: 0.0889\n",
      "😭: 0.0831\n",
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1234\n",
      "🎇: 0.0974\n",
      "😭: 0.0859\n",
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1209\n",
      "🎇: 0.0924\n",
      "😭: 0.0851\n",
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1234\n",
      "🎇: 0.0974\n",
      "😭: 0.0859\n",
      "\n",
      "预测的表情: 得意\n",
      "\n",
      "前3个最可能的表情:\n",
      "得意: 0.1215\n",
      "🎇: 0.0916\n",
      "😭: 0.0837\n"
     ]
    }
   ],
   "source": [
    "class EmojiPredictor:\n",
    "    def __init__(self, model_path, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        # 加载模型和配置\n",
    "        self.device = device\n",
    "        self.config = GPTConfig()\n",
    "        self.model = GPT(self.config)\n",
    "        \n",
    "        # 加载保存的模型权重\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        if 'module.state_dict' in checkpoint:\n",
    "            # 如果是从DDP模型保存的\n",
    "            self.model.load_state_dict({k[7:]: v for k, v in checkpoint['model_state_dict'].items()})\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            \n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 初始化tokenizer\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # 从训练数据中获取emoji映射\n",
    "        self.dataset = MyDataset(\"dataset.json\")\n",
    "        self.emoji_mapping = self.dataset.get_emoji_mapping()\n",
    "        self.reverse_emoji_mapping = {v: k for k, v in self.emoji_mapping.items()}\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        预测输入文本对应的表情\n",
    "        参数:\n",
    "            text: 输入文本\n",
    "        返回:\n",
    "            predicted_emoji: 预测的表情\n",
    "            probabilities: 每个表情的概率分布\n",
    "        \"\"\"\n",
    "        # 对输入文本进行编码\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "        \n",
    "        # 处理序列长度\n",
    "        if len(token_ids) > self.config.block_size:\n",
    "            token_ids = token_ids[:self.config.block_size]\n",
    "        else:\n",
    "            token_ids = token_ids + [self.tokenizer.eot_token] * (self.config.block_size - len(token_ids))\n",
    "        \n",
    "        # 转换为tensor并移到对应设备\n",
    "        token_ids = torch.tensor(token_ids).unsqueeze(0).to(self.device)  # 添加batch维度\n",
    "        \n",
    "        # 进行预测\n",
    "        with torch.no_grad():\n",
    "            logits, _ = self.model(token_ids)\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "            \n",
    "        # 获取预测的表情\n",
    "        predicted_emoji = self.reverse_emoji_mapping[predicted_class]\n",
    "        \n",
    "        # 获取所有表情的概率分布\n",
    "        prob_dict = {}\n",
    "        probs = probabilities[0].cpu().numpy()\n",
    "        for emoji_class in range(len(self.emoji_mapping)):\n",
    "            emoji = self.reverse_emoji_mapping[emoji_class]\n",
    "            prob_dict[emoji] = float(probs[emoji_class])\n",
    "        \n",
    "        return predicted_emoji, prob_dict\n",
    "\n",
    "    def predict_top_k(self, text, k=3):\n",
    "        \"\"\"\n",
    "        预测输入文本对应的前k个最可能的表情\n",
    "        \"\"\"\n",
    "        _, prob_dict = self.predict(text)\n",
    "        # 按概率排序并返回前k个\n",
    "        sorted_emojis = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return sorted_emojis\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    # 初始化预测器\n",
    "    predictor = EmojiPredictor('model_epoch_3.pt')  # 使用最后一个epoch的模型\n",
    "    \n",
    "    # 交互式预测\n",
    "    while True:\n",
    "        text = input(\"\\n请输入文本 (输入'quit'退出): \")\n",
    "        if text.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        # 获取预测结果\n",
    "        emoji, probs = predictor.predict(text)\n",
    "        print(f\"\\n预测的表情: {emoji}\")\n",
    "        \n",
    "        # 显示前3个最可能的表情及其概率\n",
    "        top_3 = predictor.predict_top_k(text, k=3)\n",
    "        print(\"\\n前3个最可能的表情:\")\n",
    "        for emoji, prob in top_3:\n",
    "            print(f\"{emoji}: {prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
